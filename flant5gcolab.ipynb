{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45625e3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import os\n",
    "\n",
    "model_name = \"google/flan-t5-small\"\n",
    "local_dir = \"./models/flan\"\n",
    "\n",
    "print(\"Downloading...\")\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Save model\n",
    "os.makedirs(local_dir, exist_ok=True)\n",
    "print(f\"Saving model to {local_dir}\")\n",
    "tokenizer.save_pretrained(local_dir)\n",
    "model.save_pretrained(local_dir)\n",
    "\n",
    "# Load model\n",
    "print(\"Loading model from local directory...\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(local_dir)\n",
    "model = T5ForConditionalGeneration.from_pretrained(local_dir)\n",
    "\n",
    "# Testing\n",
    "while True:\n",
    "    prompt = input(\"Enter a prompt (or type 'exit' to stop): \")\n",
    "    if prompt.lower() == \"exit\":\n",
    "        break\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "\n",
    "    print(\"Generating response...\")\n",
    "    outputs = model.generate(**inputs, max_new_tokens=50)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    print(\"\\nGenerated Output:\")\n",
    "    print(response)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
