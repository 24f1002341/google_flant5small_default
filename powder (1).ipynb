{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ax_M1ED_0lOb"
      },
      "outputs": [],
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "import os\n",
        "import torch\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "model_name = \"google/flan-t5-small\"\n",
        "local_dir = \"/content/flan_model\"\n",
        "drive_dir = \"/content/drive/MyDrive/flan_model\"\n",
        "\n",
        "print(\"Downloading model and tokenizer...\")\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "os.makedirs(local_dir, exist_ok=True)\n",
        "tokenizer.save_pretrained(local_dir)\n",
        "model.save_pretrained(local_dir)\n",
        "print(f\"✅ Model saved to {local_dir}\")\n",
        "\n",
        "\n",
        "os.makedirs(drive_dir, exist_ok=True)\n",
        "tokenizer.save_pretrained(drive_dir)\n",
        "model.save_pretrained(drive_dir)\n",
        "print(f\"✅ Model also saved to {drive_dir}\")\n",
        "\n",
        "print(\"Loading model from local directory...\")\n",
        "tokenizer = T5Tokenizer.from_pretrained(local_dir)\n",
        "model = T5ForConditionalGeneration.from_pretrained(local_dir)\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "print(f\"✅ Using device: {device}\")\n",
        "\n",
        "\n",
        "prompt = \"summarize: Machine learning is a subfield of artificial intelligence concerned with algorithms that learn from data.\"\n",
        "inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
        "\n",
        "print(\"Generating response...\")\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=50,\n",
        "    do_sample=True,\n",
        "    temperature=0.7,\n",
        "    top_p=0.9\n",
        ")\n",
        "\n",
        "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(\"\\nGenerated Output:\")\n",
        "print(response)\n",
        "\n",
        "\n",
        "while True:\n",
        "    try:\n",
        "        user_prompt = input(\"\\nEnter a prompt (or type 'exit' to stop): \")\n",
        "        if user_prompt.lower() == \"exit\":\n",
        "            print(\"Exiting.\")\n",
        "            break\n",
        "\n",
        "        if not user_prompt.strip():\n",
        "            print(\"Empty prompt. Try again.\")\n",
        "            continue\n",
        "\n",
        "        inputs = tokenizer(user_prompt, return_tensors='pt').to(device)\n",
        "\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=50,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9\n",
        "        )\n",
        "\n",
        "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        print(\"Response:\")\n",
        "        print(response)\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nInterrupted. Exiting.\")\n",
        "        break\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "for fine tuning **flant5**"
      ],
      "metadata": {
        "id": "ttWG1cqllg7z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "OLuaU4SQm-Ep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import T5Tokenizer,T5ForConditionalGeneration\n",
        "import transformers\n",
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "\n",
        "\n",
        "#load dataset\n",
        "dataset= load_dataset('json',data_files='/content/drive/MyDrive/data.json')\n",
        "\n",
        "dataset=dataset[\"train\"].train_test_split(test_size=0.1)\n",
        "\n",
        "#tokenise dataset\n",
        "\n",
        "tokenizer=T5Tokenizer.from_pretrained('/content/drive/MyDrive/flan_model')\n",
        "\n",
        "def tokenize(batch):\n",
        "    input_enc=tokenizer(batch[\"input\"],padding=\"max_length\",truncation=True,max_length=128)\n",
        "    output_enc=tokenizer(batch[\"output\"],padding=\"max_length\",truncation=True,max_length=64)\n",
        "\n",
        "\n",
        "    labels = output_enc[\"input_ids\"]\n",
        "    labels = [[(token if token != tokenizer.pad_token_id else -100) for token in label_seq] for label_seq in labels]\n",
        "\n",
        "    input_enc[\"labels\"] = labels\n",
        "    return input_enc\n",
        "tokenized_dataset = dataset.map(tokenize, batched=True)\n",
        "\n",
        "#fine tuning\n",
        "\n",
        "model=T5ForConditionalGeneration.from_pretrained('google/flan-t5-small')\n",
        "args=transformers.TrainingArguments(\n",
        "    output_dir=\"/content/drive/MyDrive/flan_model_finetuned\",\n",
        "    run_name=\"flan_finetune_run\",\n",
        "    per_device_train_batch_size=10,\n",
        "    per_device_eval_batch_size=6,\n",
        "    num_train_epochs=20,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_total_limit=2,\n",
        "    fp16=True,\n",
        "    logging_dir=\"./logs\",\n",
        "    remove_unused_columns=False,\n",
        "\n",
        ")\n",
        "\n",
        "print('machine is learning')\n",
        "\n",
        "trainer=transformers.Trainer(model=model,\n",
        "                args=args,\n",
        "                train_dataset=tokenized_dataset[\"train\"],\n",
        "                eval_dataset=tokenized_dataset['test'])\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "trainer.save_model(\"/content/drive/MyDrive/flan_model_finetuned/final_model\")\n",
        "tokenizer.save_pretrained(\"/content/drive/MyDrive/flan_model_finetuned/final_model\")"
      ],
      "metadata": {
        "id": "uyWIduBSlwpe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "testing the pretrained model"
      ],
      "metadata": {
        "id": "mO8mXFjWt4Zq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from transformers import pipeline, T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "model_path = \"/content/drive/MyDrive/flan_model_finetuned/final_model\"\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_path, local_files_only=True)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_path, local_files_only=True)\n",
        "\n",
        "# Create the pipeline with the loaded model and tokenizer\n",
        "pipe = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "\n",
        "\n",
        "while True:\n",
        "    prompt=input('>>>>')\n",
        "    if prompt.lower() =='exit':\n",
        "        break\n",
        "    out=pipe(prompt,max_new_tokens=50)\n",
        "    print(\"\\n Response:\", out[0]['generated_text'])"
      ],
      "metadata": {
        "id": "4hOW5o72uBAE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}